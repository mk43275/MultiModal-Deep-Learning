{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Assignment 3**\n",
        "\n",
        "### Overview:\n",
        "In this assignment, you will explore the field of time series forecasting using deep learning techniques. The importance of this task lies in its real-world applications, such as predicting energy consumption, stock prices, and more. You will work with synthetic training and testing datasets to create, train, and evaluate feedforward neural networks using PyTorch. The assignment consists of several key tasks that build upon each other, with opportunities for feature engineering, model architecture design, and regularization techniques.\n",
        "\n",
        "Dataset: I have created the training and testing datasets synthetically to mimic real-world energy consumption data. These datasets have been provided in the assignment folder\n",
        "\n",
        "Training Data: Canvas->Files>Assigment3->energy_consumption_train.csv\n",
        "Test Data: Canvas->Files>Assigment3->energy_consumption_test.csv\n",
        "Tasks:\n",
        "\n",
        "###1. Feature Engineering (2 points):\n",
        "In this section, you will be required to create meaningful features from the provided DATE column in the training and testing datasets. You should consider features like year, month, day of the week, day of the month, public holidays, and any other features that you find relevant. Feature engineering is open-ended, and students are encouraged to implement features based on a suggested list. No wrong answers so no penalty as long as you implement a set of basic features (should be more than or equal to three features)\n",
        "\n"
      ],
      "metadata": {
        "id": "2KNsAEyUFoFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''#importing necessary libraries'''\n",
        "#for dataframe\n",
        "import pandas as pd\n",
        "import calendar #for day of week\n",
        "import holidays #holidays with the dates\n",
        "\n",
        "#scaling & one-hot & mse/msa\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "#pytorch for nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "#plot raw images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dwDtY904Z_Zm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOyEbtYlFnOO",
        "outputId": "4a1c043f-5feb-4f48-d201-9d217c9e8d90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Energy_Consumption_kWh\n",
            "0  2022-04-11               600.45796\n",
            "1  2020-03-13               435.67424\n",
            "2  2021-07-21               404.28419\n",
            "         Date  Energy_Consumption_kWh\n",
            "0  2022-10-30              723.311422\n",
            "1  2021-12-08              375.930672\n",
            "2  2022-01-13              642.631686\n"
          ]
        }
      ],
      "source": [
        "'''Feature Engineering'''\n",
        "#load csv into pd df\n",
        "train_df = pd.read_csv('energytrain.csv')\n",
        "test_df = pd.read_csv('energytest.csv')\n",
        "\n",
        "#print first 3 in both df\n",
        "print(train_df.head(3))\n",
        "print(test_df.head(3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turning date into datetime format\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "\n",
        "#list\n",
        "dfs = [train_df, test_df]\n",
        "\n",
        "for df in dfs:\n",
        "    df['y'] = df['Date'].dt.year\n",
        "    df['m'] = df['Date'].dt.month\n",
        "    df['d'] = df['Date'].dt.day\n",
        "    df['day_of_week'] = df['Date'].dt.weekday  #monday=0, ..., sunday=6\n",
        "    #df['DayName'] = df['Date'].dt.day_name()  #'Monday', 'Tuesday', etc.\n",
        "\n",
        "    #holiday (1 if yes, 0 if no)\n",
        "    us_holidays = holidays.US()\n",
        "    df['holiday'] = df['Date'].apply(lambda x: 1 if x in us_holidays else 0)\n",
        "\n",
        "#drop original column\n",
        "train_df.drop(columns=['Date'], inplace=True)\n",
        "test_df.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "#print & check\n",
        "print(train_df.head(3))\n",
        "print(test_df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bysy37f9eJWW",
        "outputId": "b7be4e6e-6b1a-4c17-da05-94f6dc6d6e2b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Energy_Consumption_kWh     y  m   d  day_of_week  holiday\n",
            "0               600.45796  2022  4  11            0        0\n",
            "1               435.67424  2020  3  13            4        0\n",
            "2               404.28419  2021  7  21            2        0\n",
            "   Energy_Consumption_kWh     y   m   d  day_of_week  holiday\n",
            "0              723.311422  2022  10  30            6        0\n",
            "1              375.930672  2021  12   8            2        0\n",
            "2              642.631686  2022   1  13            3        0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#separating target\n",
        "y_train = train_df[['Energy_Consumption_kWh']].values.astype('float32')\n",
        "y_test = test_df[['Energy_Consumption_kWh']].values.astype('float32')"
      ],
      "metadata": {
        "id": "S1OjWUTvm9gT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SCALING\n",
        "scaler = StandardScaler()\n",
        "target_scaler = StandardScaler()\n",
        "\n",
        "numeric = ['y', 'm', 'd']\n",
        "train_df[numeric] = scaler.fit_transform(train_df[numeric]).astype('float32')\n",
        "test_df[numeric] = scaler.transform(test_df[numeric]).astype('float32')\n",
        "\n",
        "y_train = target_scaler.fit_transform(y_train).astype('float32')\n",
        "y_test = target_scaler.transform(y_test).astype('float32')"
      ],
      "metadata": {
        "id": "fXWFl6uCmciZ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ONE-HOT\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') #ignore unseen categories in test\n",
        "\n",
        "categorical_features = ['day_of_week']\n",
        "train_cat_encoded = encoder.fit_transform(train_df[categorical_features]).astype('float32')\n",
        "test_cat_encoded = encoder.transform(test_df[categorical_features]).astype('float32')\n",
        "\n",
        "#convert encoded categorical features to DataFrame\n",
        "train_cat_df = pd.DataFrame(train_cat_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
        "test_cat_df = pd.DataFrame(test_cat_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
        "\n",
        "#reset indices to align\n",
        "train_cat_df.index = train_df.index\n",
        "test_cat_df.index = test_df.index\n",
        "\n",
        "#concatenate encoded categorical features back to dataset & drop what we changed\n",
        "train_df = pd.concat([train_df.drop(columns=categorical_features), train_cat_df], axis=1)\n",
        "test_df = pd.concat([test_df.drop(columns=categorical_features), test_cat_df], axis=1)\n",
        "\n",
        "#convert to PyTorch tensors\n",
        "X_train = torch.tensor(train_df.drop(columns=['Energy_Consumption_kWh']).values, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor(test_df.drop(columns=['Energy_Consumption_kWh']).values, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "#create pytorch data loader\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#print to verify\n",
        "print(\"X_train shape:\", X_train.shape, \"| y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape, \"| y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYTejzmLmd6E",
        "outputId": "951ae5f2-65eb-474d-af06-dde3fa904f4b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([8000, 11]) | y_train shape: torch.Size([8000, 1])\n",
            "X_test shape: torch.Size([2000, 11]) | y_test shape: torch.Size([2000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Neural Network Design (2 points):\n",
        "You will design a feedforward neural network using PyTorch and the nn.Module framework. This network will serve as your baseline model for energy consumption forecasting. You'll need to write the training and testing code and report two key metrics: Mean Squared Error (MSE) and Mean Absolute Error (MAE). Explain what these metrics represent and their significance in evaluating the model's performance.\n",
        "\n",
        "Code References for metrics\n",
        "\n",
        "1. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
        "\n",
        "2. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n",
        "\n"
      ],
      "metadata": {
        "id": "n5NYsKpMF6XV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Neural Network Design'''\n",
        "# define the neural network - baseline\n",
        "class BaseEnergyConsumptionNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(BaseEnergyConsumptionNN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.relu3(self.fc3(x))\n",
        "        return self.fc4(x)\n"
      ],
      "metadata": {
        "id": "KdqMhHZTF75f"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Modification with Dropout (2 points):\n",
        "Building upon your baseline model, you will modify the neural network by adding dropout layers at several points in the architecture. Train this modified network using the training data and report the evaluation metrics (MSE and MAE). Share your observations and insights regarding the impact of dropout on model performance.\n"
      ],
      "metadata": {
        "id": "qI-iy_c1F8HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding dropout\n",
        "class EnergyConsumptionNN_Dropout(nn.Module):\n",
        "    def __init__(self, input_size, dropout_rate=0.2):\n",
        "        super(EnergyConsumptionNN_Dropout, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.drop1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.drop2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.drop3 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.drop1(self.relu1(self.fc1(x)))\n",
        "        x = self.drop2(self.relu2(self.fc2(x)))\n",
        "        x = self.drop3(self.relu3(self.fc3(x)))\n",
        "        return self.fc4(x)\n"
      ],
      "metadata": {
        "id": "1RCpu_K6bfGW"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training function - without l1/l2 regularization\n",
        "def train_model(model, train_loader, num_epochs=100, loss_threshold=0.1):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_X)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"epoch [{epoch+1}/{num_epochs}] | train loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if avg_loss < loss_threshold:\n",
        "            print(f\"stopping early at epoch {epoch+1} due to low training loss.\")\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "Tvq2YaFAZaj_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###4. Regularization with L1 and L2 (2 points):\n",
        "Extend your modified network further by implementing both L1 and L2 regularization techniques on the error. Retrain the network with dropout layers and regularization and report the resulting metrics (MSE and MAE). Provide insights into how regularization affects the model's performance and whether it helps mitigate overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "kJ5-s0oiF-AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training with l1 & l2\n",
        "def train_model_with_regularization(model, train_loader, num_epochs=100, loss_threshold=0.1, l1_lambda=0.001, l2_lambda=0.001):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(batch_X)\n",
        "            loss = criterion(predictions, batch_y)\n",
        "\n",
        "            # apply L1 & L2 regularization\n",
        "            l1_reg = sum(torch.norm(p, 1) for p in model.parameters()) * l1_lambda\n",
        "            l2_reg = sum(torch.norm(p, 2) for p in model.parameters()) * l2_lambda\n",
        "            loss += l1_reg + l2_reg\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        print(f\"epoch [{epoch+1}/{num_epochs}] | train loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if avg_loss < loss_threshold:\n",
        "            print(f\"stopping early at epoch {epoch+1} due to low training loss.\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "aDJIpFJRb_uJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate models\n",
        "input_size = X_train.shape[1]\n",
        "baseline_model = BaseEnergyConsumptionNN(input_size)\n",
        "dropout_model = EnergyConsumptionNN_Dropout(input_size)\n",
        "dropout_model_l1l2 = EnergyConsumptionNN_Dropout(input_size)  # same dropout model but trained with L1 & L2\n",
        "\n",
        "# train baseline model (no dropout, no L1/L2)\n",
        "print(\"\\ntraining baseline model:\")\n",
        "train_model(baseline_model, train_loader)\n",
        "\n",
        "# train dropout model (no L1/L2)\n",
        "print(\"\\ntraining dropout model:\")\n",
        "train_model(dropout_model, train_loader)\n",
        "\n",
        "# train dropout model with L1 & L2\n",
        "print(\"\\ntraining dropout model with L1 & L2:\")\n",
        "train_model_with_regularization(dropout_model_l1l2, train_loader)\n",
        "\n",
        "# evaluate models\n",
        "models = {\n",
        "    \"Baseline (No Dropout, No L1/L2)\": baseline_model,\n",
        "    \"Dropout Model (No L1/L2)\": dropout_model,\n",
        "    \"Dropout Model (With L1 & L2)\": dropout_model_l1l2\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test).numpy()\n",
        "    mse = mean_squared_error(y_test.numpy(), y_pred)\n",
        "    mae = mean_absolute_error(y_test.numpy(), y_pred)\n",
        "    results[name] = (mse, mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljibIZ7Lb_JI",
        "outputId": "42bafb78-567e-4aea-d498-4211f78628f0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "training baseline model:\n",
            "epoch [1/100] | train loss: 1.0019\n",
            "epoch [2/100] | train loss: 1.0004\n",
            "epoch [3/100] | train loss: 0.9991\n",
            "epoch [4/100] | train loss: 0.9976\n",
            "epoch [5/100] | train loss: 0.9979\n",
            "epoch [6/100] | train loss: 0.9968\n",
            "epoch [7/100] | train loss: 0.9950\n",
            "epoch [8/100] | train loss: 0.9945\n",
            "epoch [9/100] | train loss: 0.9932\n",
            "epoch [10/100] | train loss: 0.9922\n",
            "epoch [11/100] | train loss: 0.9912\n",
            "epoch [12/100] | train loss: 0.9904\n",
            "epoch [13/100] | train loss: 0.9889\n",
            "epoch [14/100] | train loss: 0.9881\n",
            "epoch [15/100] | train loss: 0.9875\n",
            "epoch [16/100] | train loss: 0.9864\n",
            "epoch [17/100] | train loss: 0.9864\n",
            "epoch [18/100] | train loss: 0.9850\n",
            "epoch [19/100] | train loss: 0.9844\n",
            "epoch [20/100] | train loss: 0.9828\n",
            "epoch [21/100] | train loss: 0.9833\n",
            "epoch [22/100] | train loss: 0.9808\n",
            "epoch [23/100] | train loss: 0.9810\n",
            "epoch [24/100] | train loss: 0.9803\n",
            "epoch [25/100] | train loss: 0.9802\n",
            "epoch [26/100] | train loss: 0.9792\n",
            "epoch [27/100] | train loss: 0.9776\n",
            "epoch [28/100] | train loss: 0.9766\n",
            "epoch [29/100] | train loss: 0.9757\n",
            "epoch [30/100] | train loss: 0.9748\n",
            "epoch [31/100] | train loss: 0.9745\n",
            "epoch [32/100] | train loss: 0.9739\n",
            "epoch [33/100] | train loss: 0.9739\n",
            "epoch [34/100] | train loss: 0.9715\n",
            "epoch [35/100] | train loss: 0.9745\n",
            "epoch [36/100] | train loss: 0.9714\n",
            "epoch [37/100] | train loss: 0.9716\n",
            "epoch [38/100] | train loss: 0.9696\n",
            "epoch [39/100] | train loss: 0.9685\n",
            "epoch [40/100] | train loss: 0.9681\n",
            "epoch [41/100] | train loss: 0.9689\n",
            "epoch [42/100] | train loss: 0.9688\n",
            "epoch [43/100] | train loss: 0.9665\n",
            "epoch [44/100] | train loss: 0.9656\n",
            "epoch [45/100] | train loss: 0.9647\n",
            "epoch [46/100] | train loss: 0.9653\n",
            "epoch [47/100] | train loss: 0.9644\n",
            "epoch [48/100] | train loss: 0.9647\n",
            "epoch [49/100] | train loss: 0.9636\n",
            "epoch [50/100] | train loss: 0.9619\n",
            "epoch [51/100] | train loss: 0.9614\n",
            "epoch [52/100] | train loss: 0.9610\n",
            "epoch [53/100] | train loss: 0.9606\n",
            "epoch [54/100] | train loss: 0.9606\n",
            "epoch [55/100] | train loss: 0.9599\n",
            "epoch [56/100] | train loss: 0.9590\n",
            "epoch [57/100] | train loss: 0.9590\n",
            "epoch [58/100] | train loss: 0.9585\n",
            "epoch [59/100] | train loss: 0.9579\n",
            "epoch [60/100] | train loss: 0.9561\n",
            "epoch [61/100] | train loss: 0.9578\n",
            "epoch [62/100] | train loss: 0.9561\n",
            "epoch [63/100] | train loss: 0.9557\n",
            "epoch [64/100] | train loss: 0.9550\n",
            "epoch [65/100] | train loss: 0.9536\n",
            "epoch [66/100] | train loss: 0.9521\n",
            "epoch [67/100] | train loss: 0.9532\n",
            "epoch [68/100] | train loss: 0.9535\n",
            "epoch [69/100] | train loss: 0.9530\n",
            "epoch [70/100] | train loss: 0.9521\n",
            "epoch [71/100] | train loss: 0.9524\n",
            "epoch [72/100] | train loss: 0.9530\n",
            "epoch [73/100] | train loss: 0.9509\n",
            "epoch [74/100] | train loss: 0.9503\n",
            "epoch [75/100] | train loss: 0.9520\n",
            "epoch [76/100] | train loss: 0.9513\n",
            "epoch [77/100] | train loss: 0.9503\n",
            "epoch [78/100] | train loss: 0.9481\n",
            "epoch [79/100] | train loss: 0.9501\n",
            "epoch [80/100] | train loss: 0.9496\n",
            "epoch [81/100] | train loss: 0.9483\n",
            "epoch [82/100] | train loss: 0.9452\n",
            "epoch [83/100] | train loss: 0.9470\n",
            "epoch [84/100] | train loss: 0.9474\n",
            "epoch [85/100] | train loss: 0.9474\n",
            "epoch [86/100] | train loss: 0.9482\n",
            "epoch [87/100] | train loss: 0.9465\n",
            "epoch [88/100] | train loss: 0.9452\n",
            "epoch [89/100] | train loss: 0.9451\n",
            "epoch [90/100] | train loss: 0.9443\n",
            "epoch [91/100] | train loss: 0.9447\n",
            "epoch [92/100] | train loss: 0.9434\n",
            "epoch [93/100] | train loss: 0.9441\n",
            "epoch [94/100] | train loss: 0.9452\n",
            "epoch [95/100] | train loss: 0.9432\n",
            "epoch [96/100] | train loss: 0.9422\n",
            "epoch [97/100] | train loss: 0.9417\n",
            "epoch [98/100] | train loss: 0.9414\n",
            "epoch [99/100] | train loss: 0.9423\n",
            "epoch [100/100] | train loss: 0.9397\n",
            "\n",
            "training dropout model:\n",
            "epoch [1/100] | train loss: 1.0018\n",
            "epoch [2/100] | train loss: 1.0015\n",
            "epoch [3/100] | train loss: 1.0007\n",
            "epoch [4/100] | train loss: 0.9991\n",
            "epoch [5/100] | train loss: 0.9990\n",
            "epoch [6/100] | train loss: 1.0002\n",
            "epoch [7/100] | train loss: 0.9997\n",
            "epoch [8/100] | train loss: 0.9990\n",
            "epoch [9/100] | train loss: 0.9987\n",
            "epoch [10/100] | train loss: 0.9988\n",
            "epoch [11/100] | train loss: 0.9982\n",
            "epoch [12/100] | train loss: 0.9986\n",
            "epoch [13/100] | train loss: 0.9974\n",
            "epoch [14/100] | train loss: 0.9975\n",
            "epoch [15/100] | train loss: 0.9968\n",
            "epoch [16/100] | train loss: 0.9974\n",
            "epoch [17/100] | train loss: 0.9960\n",
            "epoch [18/100] | train loss: 0.9968\n",
            "epoch [19/100] | train loss: 0.9960\n",
            "epoch [20/100] | train loss: 0.9954\n",
            "epoch [21/100] | train loss: 0.9945\n",
            "epoch [22/100] | train loss: 0.9957\n",
            "epoch [23/100] | train loss: 0.9962\n",
            "epoch [24/100] | train loss: 0.9952\n",
            "epoch [25/100] | train loss: 0.9949\n",
            "epoch [26/100] | train loss: 0.9939\n",
            "epoch [27/100] | train loss: 0.9939\n",
            "epoch [28/100] | train loss: 0.9942\n",
            "epoch [29/100] | train loss: 0.9934\n",
            "epoch [30/100] | train loss: 0.9908\n",
            "epoch [31/100] | train loss: 0.9925\n",
            "epoch [32/100] | train loss: 0.9925\n",
            "epoch [33/100] | train loss: 0.9920\n",
            "epoch [34/100] | train loss: 0.9911\n",
            "epoch [35/100] | train loss: 0.9909\n",
            "epoch [36/100] | train loss: 0.9910\n",
            "epoch [37/100] | train loss: 0.9899\n",
            "epoch [38/100] | train loss: 0.9896\n",
            "epoch [39/100] | train loss: 0.9889\n",
            "epoch [40/100] | train loss: 0.9888\n",
            "epoch [41/100] | train loss: 0.9868\n",
            "epoch [42/100] | train loss: 0.9886\n",
            "epoch [43/100] | train loss: 0.9889\n",
            "epoch [44/100] | train loss: 0.9860\n",
            "epoch [45/100] | train loss: 0.9861\n",
            "epoch [46/100] | train loss: 0.9850\n",
            "epoch [47/100] | train loss: 0.9880\n",
            "epoch [48/100] | train loss: 0.9872\n",
            "epoch [49/100] | train loss: 0.9844\n",
            "epoch [50/100] | train loss: 0.9864\n",
            "epoch [51/100] | train loss: 0.9848\n",
            "epoch [52/100] | train loss: 0.9866\n",
            "epoch [53/100] | train loss: 0.9823\n",
            "epoch [54/100] | train loss: 0.9847\n",
            "epoch [55/100] | train loss: 0.9828\n",
            "epoch [56/100] | train loss: 0.9840\n",
            "epoch [57/100] | train loss: 0.9827\n",
            "epoch [58/100] | train loss: 0.9810\n",
            "epoch [59/100] | train loss: 0.9804\n",
            "epoch [60/100] | train loss: 0.9836\n",
            "epoch [61/100] | train loss: 0.9816\n",
            "epoch [62/100] | train loss: 0.9782\n",
            "epoch [63/100] | train loss: 0.9815\n",
            "epoch [64/100] | train loss: 0.9828\n",
            "epoch [65/100] | train loss: 0.9845\n",
            "epoch [66/100] | train loss: 0.9801\n",
            "epoch [67/100] | train loss: 0.9784\n",
            "epoch [68/100] | train loss: 0.9811\n",
            "epoch [69/100] | train loss: 0.9805\n",
            "epoch [70/100] | train loss: 0.9767\n",
            "epoch [71/100] | train loss: 0.9782\n",
            "epoch [72/100] | train loss: 0.9763\n",
            "epoch [73/100] | train loss: 0.9791\n",
            "epoch [74/100] | train loss: 0.9771\n",
            "epoch [75/100] | train loss: 0.9786\n",
            "epoch [76/100] | train loss: 0.9769\n",
            "epoch [77/100] | train loss: 0.9764\n",
            "epoch [78/100] | train loss: 0.9811\n",
            "epoch [79/100] | train loss: 0.9787\n",
            "epoch [80/100] | train loss: 0.9762\n",
            "epoch [81/100] | train loss: 0.9776\n",
            "epoch [82/100] | train loss: 0.9761\n",
            "epoch [83/100] | train loss: 0.9743\n",
            "epoch [84/100] | train loss: 0.9742\n",
            "epoch [85/100] | train loss: 0.9761\n",
            "epoch [86/100] | train loss: 0.9764\n",
            "epoch [87/100] | train loss: 0.9769\n",
            "epoch [88/100] | train loss: 0.9779\n",
            "epoch [89/100] | train loss: 0.9715\n",
            "epoch [90/100] | train loss: 0.9720\n",
            "epoch [91/100] | train loss: 0.9755\n",
            "epoch [92/100] | train loss: 0.9734\n",
            "epoch [93/100] | train loss: 0.9765\n",
            "epoch [94/100] | train loss: 0.9708\n",
            "epoch [95/100] | train loss: 0.9747\n",
            "epoch [96/100] | train loss: 0.9722\n",
            "epoch [97/100] | train loss: 0.9721\n",
            "epoch [98/100] | train loss: 0.9689\n",
            "epoch [99/100] | train loss: 0.9703\n",
            "epoch [100/100] | train loss: 0.9704\n",
            "\n",
            "training dropout model with L1 & L2:\n",
            "epoch [1/100] | train loss: 1.5357\n",
            "epoch [2/100] | train loss: 1.2261\n",
            "epoch [3/100] | train loss: 1.1099\n",
            "epoch [4/100] | train loss: 1.0533\n",
            "epoch [5/100] | train loss: 1.0226\n",
            "epoch [6/100] | train loss: 1.0101\n",
            "epoch [7/100] | train loss: 1.0053\n",
            "epoch [8/100] | train loss: 1.0040\n",
            "epoch [9/100] | train loss: 1.0042\n",
            "epoch [10/100] | train loss: 1.0029\n",
            "epoch [11/100] | train loss: 1.0023\n",
            "epoch [12/100] | train loss: 1.0024\n",
            "epoch [13/100] | train loss: 1.0027\n",
            "epoch [14/100] | train loss: 1.0029\n",
            "epoch [15/100] | train loss: 1.0022\n",
            "epoch [16/100] | train loss: 1.0021\n",
            "epoch [17/100] | train loss: 1.0023\n",
            "epoch [18/100] | train loss: 1.0020\n",
            "epoch [19/100] | train loss: 1.0018\n",
            "epoch [20/100] | train loss: 1.0023\n",
            "epoch [21/100] | train loss: 1.0021\n",
            "epoch [22/100] | train loss: 1.0019\n",
            "epoch [23/100] | train loss: 1.0017\n",
            "epoch [24/100] | train loss: 1.0019\n",
            "epoch [25/100] | train loss: 1.0018\n",
            "epoch [26/100] | train loss: 1.0014\n",
            "epoch [27/100] | train loss: 1.0018\n",
            "epoch [28/100] | train loss: 1.0014\n",
            "epoch [29/100] | train loss: 1.0015\n",
            "epoch [30/100] | train loss: 1.0013\n",
            "epoch [31/100] | train loss: 1.0014\n",
            "epoch [32/100] | train loss: 1.0015\n",
            "epoch [33/100] | train loss: 1.0013\n",
            "epoch [34/100] | train loss: 1.0013\n",
            "epoch [35/100] | train loss: 1.0013\n",
            "epoch [36/100] | train loss: 1.0012\n",
            "epoch [37/100] | train loss: 1.0011\n",
            "epoch [38/100] | train loss: 1.0009\n",
            "epoch [39/100] | train loss: 1.0010\n",
            "epoch [40/100] | train loss: 1.0009\n",
            "epoch [41/100] | train loss: 1.0009\n",
            "epoch [42/100] | train loss: 1.0008\n",
            "epoch [43/100] | train loss: 1.0008\n",
            "epoch [44/100] | train loss: 1.0008\n",
            "epoch [45/100] | train loss: 1.0007\n",
            "epoch [46/100] | train loss: 1.0007\n",
            "epoch [47/100] | train loss: 1.0007\n",
            "epoch [48/100] | train loss: 1.0007\n",
            "epoch [49/100] | train loss: 1.0007\n",
            "epoch [50/100] | train loss: 1.0007\n",
            "epoch [51/100] | train loss: 1.0007\n",
            "epoch [52/100] | train loss: 1.0008\n",
            "epoch [53/100] | train loss: 1.0007\n",
            "epoch [54/100] | train loss: 1.0007\n",
            "epoch [55/100] | train loss: 1.0007\n",
            "epoch [56/100] | train loss: 1.0007\n",
            "epoch [57/100] | train loss: 1.0007\n",
            "epoch [58/100] | train loss: 1.0007\n",
            "epoch [59/100] | train loss: 1.0008\n",
            "epoch [60/100] | train loss: 1.0007\n",
            "epoch [61/100] | train loss: 1.0007\n",
            "epoch [62/100] | train loss: 1.0007\n",
            "epoch [63/100] | train loss: 1.0007\n",
            "epoch [64/100] | train loss: 1.0008\n",
            "epoch [65/100] | train loss: 1.0007\n",
            "epoch [66/100] | train loss: 1.0008\n",
            "epoch [67/100] | train loss: 1.0007\n",
            "epoch [68/100] | train loss: 1.0007\n",
            "epoch [69/100] | train loss: 1.0007\n",
            "epoch [70/100] | train loss: 1.0008\n",
            "epoch [71/100] | train loss: 1.0007\n",
            "epoch [72/100] | train loss: 1.0007\n",
            "epoch [73/100] | train loss: 1.0007\n",
            "epoch [74/100] | train loss: 1.0007\n",
            "epoch [75/100] | train loss: 1.0007\n",
            "epoch [76/100] | train loss: 1.0007\n",
            "epoch [77/100] | train loss: 1.0007\n",
            "epoch [78/100] | train loss: 1.0007\n",
            "epoch [79/100] | train loss: 1.0007\n",
            "epoch [80/100] | train loss: 1.0007\n",
            "epoch [81/100] | train loss: 1.0007\n",
            "epoch [82/100] | train loss: 1.0007\n",
            "epoch [83/100] | train loss: 1.0007\n",
            "epoch [84/100] | train loss: 1.0007\n",
            "epoch [85/100] | train loss: 1.0007\n",
            "epoch [86/100] | train loss: 1.0007\n",
            "epoch [87/100] | train loss: 1.0007\n",
            "epoch [88/100] | train loss: 1.0007\n",
            "epoch [89/100] | train loss: 1.0008\n",
            "epoch [90/100] | train loss: 1.0007\n",
            "epoch [91/100] | train loss: 1.0008\n",
            "epoch [92/100] | train loss: 1.0007\n",
            "epoch [93/100] | train loss: 1.0007\n",
            "epoch [94/100] | train loss: 1.0008\n",
            "epoch [95/100] | train loss: 1.0007\n",
            "epoch [96/100] | train loss: 1.0007\n",
            "epoch [97/100] | train loss: 1.0007\n",
            "epoch [98/100] | train loss: 1.0007\n",
            "epoch [99/100] | train loss: 1.0007\n",
            "epoch [100/100] | train loss: 1.0007\n",
            "\n",
            "model comparison\n",
            "Baseline (No Dropout, No L1/L2) - test mse: 1.0802, test mae: 0.8230\n",
            "Dropout Model (No L1/L2) - test mse: 1.0468, test mae: 0.8099\n",
            "Dropout Model (With L1 & L2) - test mse: 1.0245, test mae: 0.8029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print final comparison\n",
        "print(\"\\nmodel comparison\")\n",
        "for name, (mse, mae) in results.items():\n",
        "    print(f\"{name} - test mse: {mse:.4f}, test mae: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBUGqqWw2tb5",
        "outputId": "455c49e8-9f8c-480e-d562-dcb547d77eb6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "model comparison\n",
            "Baseline (No Dropout, No L1/L2) - test mse: 1.0802, test mae: 0.8230\n",
            "Dropout Model (No L1/L2) - test mse: 1.0468, test mae: 0.8099\n",
            "Dropout Model (With L1 & L2) - test mse: 1.0245, test mae: 0.8029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Insights\n",
        "\n",
        " **Baseline Model (No Dropout, No L1/L2)**\n",
        "- **Train Loss:** Steadily decreased to **0.9397**.\n",
        "- **Test MSE:** **1.0802** | **Test MAE:** **0.8230**.\n",
        "- Performs **decently**, but may be **overfitting** slightly.\n",
        "\n",
        "**Dropout Model (No L1/L2)**\n",
        "- **Train Loss:** Stabilized around **0.9704**.\n",
        "- **Test MSE:** **1.0468** | **Test MAE:** **0.8099**.\n",
        "- **Better generalization** than baseline, slight performance improvement.\n",
        "\n",
        "**Dropout Model (With L1 & L2 Regularization)**\n",
        "- **Train Loss:** Started high due to regularization but stabilized at **1.0007**.\n",
        "- **Test MSE:** **1.0245** | **Test MAE:** **0.8029**.\n",
        "- Best test performance **(lowest MSE & MAE)** --- less overfitting\n",
        "\n",
        "\n",
        "### I tried batch normalization\n",
        "\n",
        "Tried to stabilize training and improve generalization. The idea was to normalize activations, allowing the model to learn faster.\n",
        "\n",
        "- **Loss was actually higher** with BN applied.\n",
        "- The model didn't generalize better—test MSE stayed the same or worsened.\n",
        "- Removing BN resulted in **lower training loss and better performance**.\n",
        "\n",
        "### Final Takeaways\n",
        "- **Dropout + L1 & L2 is the winner** – best test error.  \n",
        "- **Dropout alone helps**, but adding L1 & L2 **reduces overfitting** even more.  \n",
        "- **Baseline overfits slightly** – good, but not the best for real-world generalization.  \n"
      ],
      "metadata": {
        "id": "crJgu9Ks1lo6"
      }
    }
  ]
}