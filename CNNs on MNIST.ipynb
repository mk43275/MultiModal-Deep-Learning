{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **initializing important libraries & dataset**"
      ],
      "metadata": {
        "id": "FMeo8MYYHr_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pandas for dataset\n",
        "import pandas as pd\n",
        "\n",
        "#modeling neural networks including CNNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#invoke various optimizers\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#loading and checking the dataset\n",
        "from tensorflow import keras\n",
        "\n",
        "#plot raw images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ea2DKN5PHra5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "#convert all data to flot32, else PyTorch will cry\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#add a channel dimension (1 for grayscale)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print (\"Rows and columns in training data inputs (i.e. images): \", x_train.shape)\n",
        "print (\"Y_actual shape: \", y_train.shape)\n",
        "\n",
        "#make a list of lists\n",
        "y_train = [[y] for y in y_train]\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(y_train)\n",
        "y_train = encoder.transform(y_train).toarray()\n",
        "\n",
        "print (\"Printing the one-hot-encoded values for the class\")\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xxQnL2pARaN4",
        "outputId": "8b67b88d-24ff-4581-9f65-a56ddb244f3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Rows and columns in training data inputs (i.e. images):  (60000, 28, 28, 1)\n",
            "Y_actual shape:  (60000,)\n",
            "Printing the one-hot-encoded values for the class\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we make minibatches in a data loader and we specify the size of the minibatch here.\n",
        "batch_size = 32\n",
        "\n",
        "#convert the numpy arrays into torch tensors\n",
        "x_train = torch.tensor(x_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "\n",
        "# Torch expects the format to be (# Channels, Height, Width format)\n",
        "x_train = x_train.permute(0, 3, 1, 2) #batch, channel, height, width reshaping\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "8zx9N74TUxMM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise E1. Define the following CNN network and train on GPU\n",
        "In a separate notebook, implement the following network and train->test it.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "- **Input Layer**:\n",
        "  - The network takes grayscale images as input, where each image has a single channel (1 for grayscale).\n",
        "  \n",
        "- **Convolutional Layer 1**:\n",
        "  - Applies the first convolution operation using `nn.Conv2d(1, 32, kernel_size=5)`.\n",
        "  - Uses 32 filters with a kernel size of 5x5.\n",
        "  \n",
        "- **ReLU Activation 1**:\n",
        "  - Applies the Rectified Linear Unit (ReLU) activation function to introduce non-linearity after the first convolutional layer.\n",
        "  \n",
        "- **Max Pooling Layer 1**:\n",
        "  - Performs max-pooling with a 2x2 kernel to downsample the feature maps obtained from the first convolutional layer.\n",
        "  \n",
        "- **Convolutional Layer 2**:\n",
        "  - Applies the second convolution operation using `nn.Conv2d(32, 64, kernel_size=5)`.\n",
        "  - Uses 64 filters with a kernel size of 5x5.\n",
        "  \n",
        "- **ReLU Activation 2**:\n",
        "  - Applies the ReLU activation function after the second convolutional layer.\n",
        "  \n",
        "- **Max Pooling Layer 2**:\n",
        "  - Performs max-pooling with a 2x2 kernel to downsample the feature maps obtained from the second convolutional layer.\n",
        "  \n",
        "- **Fully Connected Layer 1 (fc1)**:\n",
        "  - Reshapes the 64x4x4 feature maps into a flattened vector.\n",
        "  - Connects to a fully connected layer with 128 neurons.\n",
        "  \n",
        "- **ReLU Activation 3**:\n",
        "  - Applies the ReLU activation function after the first fully connected layer.\n",
        "  \n",
        "- **Fully Connected Layer 2 (fc2)**:\n",
        "  - Connects to a fully connected layer with 10 neurons, corresponding to the 10 possible classes in the MNIST dataset (digits 0-9).\n",
        "  \n",
        "- **Output Layer**:\n",
        "  - The output layer provides the final classification results.\n",
        "  - The network predicts class probabilities for each of the 10 classes using a softmax activation function.\n",
        "  \n",
        "- **Forward Pass**:\n",
        "  - In the `forward` method, the input data `x` is passed through each layer sequentially.\n",
        "  - Convolutions, ReLU activations, and max-pooling operations are applied in the specified order.\n",
        "  - After max-pooling, the feature maps are flattened into a vector.\n",
        "  - The flattened vector is passed through the fully connected layers with ReLU activations.\n",
        "  - Finally, the network produces class scores as output.\n",
        "\n",
        "  Once the network is trained, print the final training loss, then compute accuracy on the text data.\n",
        "\n"
      ],
      "metadata": {
        "id": "KHzfHGMFHZvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Architecture'''\n",
        "class CNNNetwork(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNNNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv1_act = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.conv2_act = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.conv1_act(self.conv1(x)))\n",
        "        x = self.pool2(self.conv2_act(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.act3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6Ekpd7aaH0LT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create an instance of the network\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNNetwork().to(device)  # Move the model to the appropriate device\n",
        "\n",
        "#print the model architecture\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "idfASqKxU0Br",
        "outputId": "2ef9c706-44f8-46ae-9b5d-36927f3554d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNNetwork(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv1_act): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_act): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (act3): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the model, loss function, and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#train the model for 10 epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)  # Ensure data is on the same device as the model\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # forward pass\n",
        "        loss = criterion(outputs, labels)  # error / loss computation\n",
        "        loss.backward()  # gradient Computation\n",
        "        optimizer.step()  # backpropagation\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "#save the final model\n",
        "torch.save(model.state_dict(), 'mnist_cnn_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W78qhX17dZzE",
        "outputId": "fa23bacd-8b97-4182-8b8a-7c9ee0ea77af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [1875/1875], Loss: 0.0496\n",
            "Epoch [2/10], Step [1875/1875], Loss: 0.1055\n",
            "Epoch [3/10], Step [1875/1875], Loss: 0.0088\n",
            "Epoch [4/10], Step [1875/1875], Loss: 0.0135\n",
            "Epoch [5/10], Step [1875/1875], Loss: 0.0002\n",
            "Epoch [6/10], Step [1875/1875], Loss: 0.0023\n",
            "Epoch [7/10], Step [1875/1875], Loss: 0.0084\n",
            "Epoch [8/10], Step [1875/1875], Loss: 0.0066\n",
            "Epoch [9/10], Step [1875/1875], Loss: 0.0002\n",
            "Epoch [10/10], Step [1875/1875], Loss: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "for i, image in enumerate(x_test):\n",
        "    image = image.reshape(1, 1, 28, 28)  # batch_size, channel, height, width\n",
        "    image = torch.Tensor(image).to(device)  # Ensure test data is on the same device\n",
        "    output = model(image)[0]  # since batch size is 1, we will get only one output\n",
        "    y_pred = torch.argmax(output).item()\n",
        "    y_actual = y_test[i]\n",
        "    if y_actual == y_pred:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "print (f\"Accuracy = {correct / total * 100:.2f} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-VmJTT0xdeqQ",
        "outputId": "1f288739-0733-488a-8384-b436a41a8203"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 98.58 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN model trained on the MNIST dataset achieved 98.58% accuracy on the test set, indicating strong performance.\n",
        "The training loss decreased steadily across 10 epochs, with the final loss values reaching near-zero, suggesting efficient learning.\n",
        "Observations:\n",
        "\n",
        "The model architecture is well-structured, with two convolutional layers, max-pooling, and fully connected layers, allowing for hierarchical feature extraction.\n",
        "The use of ReLU activation in convolutional and fully connected layers ensures non-linearity, improving model expressiveness.\n",
        "The Adam optimizer was chosen for training, which is a good default optimizer due to its adaptive learning rate capability.\n",
        "\n",
        "The final loss values are extremely low (e.g., 0.0002 at epoch 9), which may indicate overfitting to the training data.\n",
        "The lack of dropout or weight regularization suggests the model could have poor generalization to slightly altered images."
      ],
      "metadata": {
        "id": "qmOAXF78_brI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise E2. Design a modified version of E1's model.\n",
        "\n",
        "1. Introduce a dropout layer (between with a specified dropout probability (e.g., `dropout_prob=0.2`) between  the Fully Connected Layer 1 and Fully Connected Layer 2. Dropout helps prevent overfitting by randomly setting a fraction of the input units to zero during each forward pass.\n",
        "\n",
        "**Hint:** inside __init__(), define a dropout layer using `self.dropout = nn.Dropout(p=0.5)` and call it within the `forward()` function after calling a layer to apply dropout to that layer.\n",
        "\n",
        "2. Create an instance of the modified model with dropout. Copy the training pipeline code and testing code  and repeat the training and testing process. Do you see any improvement in test accuracy after training with dropout?\n",
        "\n",
        "3. Apply L1 and L2 regularizations following the advise here: https://stackoverflow.com/questions/44641976/pytorch-how-to-add-l1-regularizer-to-activations\n",
        "\n",
        "4. Play with  hyper-parameters such as learning rate, dropout probabilities and also feel free increase the number of epochs. Are the results improving? Write down your insights in a markdown block."
      ],
      "metadata": {
        "id": "fjb0Ubn5HboL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#modified CNN with Dropout\n",
        "class ModifiedCNN(nn.Module):\n",
        "    def __init__(self, dropout_prob=0.5):\n",
        "        super(ModifiedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)  #apply Dropout\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # dropout applied\n",
        "        out = self.fc2(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# reate model instance\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModifiedCNN(dropout_prob=0.5).to(device)\n"
      ],
      "metadata": {
        "id": "XwUqiX27hdss"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the model, loss function, and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lambda1, lambda2 = 0.001, 0.001  # L1 & L2 penalty factors\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#fefine loss threshold\n",
        "loss_threshold = 0.38\n",
        "epoch = 0\n",
        "running_loss = float('inf')\n",
        "\n",
        "#train until loss goes below the threshold\n",
        "while running_loss > loss_threshold:\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    epoch += 1  #track number of epochs\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs, _ = model(images)  #forward pass\n",
        "        loss = criterion(outputs, labels)  #compute cross-entropy loss\n",
        "\n",
        "        #compute L1 & L2 Regularization\n",
        "        l1_regularization = lambda1 * sum(torch.norm(p, 1) for p in model.parameters())\n",
        "        l2_regularization = lambda2 * sum(torch.norm(p, 2) for p in model.parameters())\n",
        "\n",
        "        #total loss = Cross Entropy + L1 + L2 (stackoverflow)\n",
        "        loss += l1_regularization + l2_regularization\n",
        "\n",
        "        loss.backward()  #gradient\n",
        "        optimizer.step()  #backprop\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    running_loss /= len(train_loader)  #average loss per epoch\n",
        "    print(f'Epoch [{epoch}], Loss: {running_loss:.4f}')\n",
        "\n",
        "#save the final model\n",
        "torch.save(model.state_dict(), 'mnist_cnn_dropout_model.pth')\n",
        "print(f\"Training completed in {epoch} epochs!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y-gKzxYqheGc",
        "outputId": "189ecdcc-172d-4604-c584-5487cad8d2c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], Loss: 1.2139\n",
            "Epoch [2], Loss: 0.4912\n",
            "Epoch [3], Loss: 0.4521\n",
            "Epoch [4], Loss: 0.4486\n",
            "Epoch [5], Loss: 0.4459\n",
            "Epoch [6], Loss: 0.4379\n",
            "Epoch [7], Loss: 0.4353\n",
            "Epoch [8], Loss: 0.4278\n",
            "Epoch [9], Loss: 0.4260\n",
            "Epoch [10], Loss: 0.4199\n",
            "Epoch [11], Loss: 0.4171\n",
            "Epoch [12], Loss: 0.4155\n",
            "Epoch [13], Loss: 0.4062\n",
            "Epoch [14], Loss: 0.4099\n",
            "Epoch [15], Loss: 0.4113\n",
            "Epoch [16], Loss: 0.4089\n",
            "Epoch [17], Loss: 0.4005\n",
            "Epoch [18], Loss: 0.4108\n",
            "Epoch [19], Loss: 0.4000\n",
            "Epoch [20], Loss: 0.4073\n",
            "Epoch [21], Loss: 0.4008\n",
            "Epoch [22], Loss: 0.4029\n",
            "Epoch [23], Loss: 0.3995\n",
            "Epoch [24], Loss: 0.4011\n",
            "Epoch [25], Loss: 0.3967\n",
            "Epoch [26], Loss: 0.3931\n",
            "Epoch [27], Loss: 0.3998\n",
            "Epoch [28], Loss: 0.3937\n",
            "Epoch [29], Loss: 0.3964\n",
            "Epoch [30], Loss: 0.4103\n",
            "Epoch [31], Loss: 0.3969\n",
            "Epoch [32], Loss: 0.4016\n",
            "Epoch [33], Loss: 0.4005\n",
            "Epoch [34], Loss: 0.3934\n",
            "Epoch [35], Loss: 0.3895\n",
            "Epoch [36], Loss: 0.4057\n",
            "Epoch [37], Loss: 0.3965\n",
            "Epoch [38], Loss: 0.3900\n",
            "Epoch [39], Loss: 0.3967\n",
            "Epoch [40], Loss: 0.3876\n",
            "Epoch [41], Loss: 0.3970\n",
            "Epoch [42], Loss: 0.3832\n",
            "Epoch [43], Loss: 0.3940\n",
            "Epoch [44], Loss: 0.3916\n",
            "Epoch [45], Loss: 0.3934\n",
            "Epoch [46], Loss: 0.3783\n",
            "Training completed in 46 epochs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i made the threshold 0.38 because i initially put it to 0.005 then 0.1, and i realized it'll never get there as epoch 80 still left me at around 0.38. I started out with 0.5 l1 penalty factor & 0.1 l2 penalty factor and that made the loss start at around 13, so i reduced it to 0.001. it helped a bit with the loss, but still not that good."
      ],
      "metadata": {
        "id": "a3OOBARplURL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('mnist_cnn_dropout_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "for i, image in enumerate(x_test):\n",
        "    image = image.reshape(1, 1, 28, 28)  # batch_size, channel, height, width\n",
        "    image = torch.Tensor(image).to(device)\n",
        "\n",
        "    output, _ = model(image)\n",
        "    y_pred = torch.argmax(output[0]).item()\n",
        "    y_actual = y_test[i]\n",
        "\n",
        "    if y_actual == y_pred:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "#print final accuracy\n",
        "print(f\"Accuracy = {correct / total * 100:.2f} %\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0n-vj7ughgBn",
        "outputId": "2d39f8de-7262-4263-c0a6-f0f12b90d2f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-f1398d00109a>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('mnist_cnn_dropout_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 98.50 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout layer (p=0.5) was used between fc1 and fc2 to reduce overfitting by randomly deactivating neurons during training.\n",
        "L1 & L2 regularization was applied to the model parameters to penalize large weights and enforce sparsity.\n",
        "Adaptive threshold-based training was used to continue training until loss fell below 0.38.\n",
        "\n",
        "The initial loss was significantly higher, especially with stronger L1 and L2 regularization settings, which had to be reduced for better convergence.\n",
        "\n",
        "Dropout prevented the model from memorizing the training set too strictly, leading to improved robustness.\n",
        "L1 and L2 regularization helped avoid extremely large weights, making the model more stable but requiring careful tuning of penalty factors.\n",
        "Hyperparameter adjustments (learning rate, dropout probability, regularization strength) were necessary to balance accuracy and training stability.\n",
        "\n",
        "## **Key Takeaways**\n",
        "The original CNN model (E1) was highly accurate but likely overfitted to the training data due to lack of regularization.\n",
        "The modified CNN (E2) introduced dropout and weight penalties, making the model more generalizable but requiring longer training.\n",
        "Despite a slight drop in test accuracy, E2's model is more robust to noise and unseen data due to dropout and regularization.\n",
        "I think the trade-off between faster convergence (E1) vs. generalization (E2) highlights the importance of regularization techniques in deep learning."
      ],
      "metadata": {
        "id": "MxYx9WoN7IQ_"
      }
    }
  ]
}